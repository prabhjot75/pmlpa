---
title: "Prediction Assignment"
author: "Prabhjot Singh"
date: "September 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
### Bakground
The goal of this project is to create a machine-learning algorithm that can correctly identify the quality of barbell lifts by using data from belt, forearm, arm, and dumbbell monitors. There are five classifications of this exercise, one method is the correct form of the exercise while the other four are common mistakes: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

### Input Data
The training data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

### Load and Clean the Data

```{r results='hide', message=FALSE, warning=FALSE}
library(caret)
library(gbm)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(knitr)

```

```{r}
set.seed(7575)

trainingDataUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingDataUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainingDataUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testingDataUrl), na.strings=c("NA","#DIV/0!",""))

#Partioning the training set into two (60-40 ratio)

inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
trainingData <- training[inTrain, ]
testingData <- training[-inTrain, ]
```

```{r results='hide', message=FALSE, warning=FALSE}
dim(trainingData); 
dim(testingData)
```

```{r}
#Clean the data

nzv <- nearZeroVar(trainingData, saveMetrics=TRUE)
trainingData <- trainingData[,nzv$nzv==FALSE]

nzv<- nearZeroVar(testingData,saveMetrics=TRUE)
testingData <- testingData[,nzv$nzv==FALSE]

#Remove the first column of the trainingData data set
trainingData <- trainingData[c(-1)]

#Clean variables with more than 60% NA
cleanTrainingData <- trainingData
for(i in 1:length(trainingData)) {
    if( sum( is.na( trainingData[, i] ) ) /nrow(trainingData) >= .7) {
        for(j in 1:length(cleanTrainingData)) {
            if( length( grep(names(trainingData[i]), names(cleanTrainingData)[j]) ) == 1)  {
                cleanTrainingData <- cleanTrainingData[ , -j]
            }   
        } 
    }
}

# Set back to the original variable name
trainingData <- cleanTrainingData
rm(cleanTrainingData)

#Transform the testingData and testing data sets

cleanData1 <- colnames(trainingData)
cleanData2 <- colnames(trainingData[, -58])  # remove the classe column
testingData <- testingData[cleanData1]         # allow only variables in testingData that are also in trainingData
testing <- testing[cleanData2]             # allow only variables in testing that are also in trainingData
```

```{r results='hide', message=FALSE, warning=FALSE}

dim(testingData)
dim(testing)

```


```{r}
#Coerce the data into the same type
for (i in 1:length(testing) ) {
    for(j in 1:length(trainingData)) {
        if( length( grep(names(trainingData[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(trainingData[i])
        }      
    }      
}

# To get the same class between testing and trainingData
testing <- rbind(trainingData[2, -58] , testing)
testing <- testing[-1,]

```

## Prediction using Decision Tree
```{r}
set.seed(7575)
modFitA1 <- rpart(classe ~ ., data=trainingData, method="class")
fancyRpartPlot(modFitA1)
predictionsA1 <- predict(modFitA1, testingData, type = "class")
cmtree <- confusionMatrix(predictionsA1, testingData$classe)
cmtree

plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))


```

## Prediction using Random Forest

```{r}
set.seed(7575)
modFitB1 <- randomForest(classe ~ ., data=trainingData)
predictionB1 <- predict(modFitB1, testingData, type = "class")
cmrf <- confusionMatrix(predictionB1, testingData$classe)
cmrf

plot(modFitB1)

plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))

```

##  Prediction using Generalized Boosted Regression

```{r message=FALSE, warning=FALSE}

set.seed(7575)
fitControl <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
gbmFit1 <- train(classe ~ ., data=trainingData, method = "gbm", trControl = fitControl, verbose = FALSE)
gbmFinMod1 <- gbmFit1$finalModel

gbmPredTest <- predict(gbmFit1, newdata=testingData)

gbmAccuracyTest <- confusionMatrix(gbmPredTest, testingData$classe)
gbmAccuracyTest

plot(gbmFit1, ylim=c(0.9, 1))

```

##  Predicting Results on the Test Data

```{r}

predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2

```

## Conclusion

Random Forest and Boosted Regression were superior models for prediction of exercise quality compared to rpart. The nominal categories were dependent on various variables and the interaction between them. The RF, BR model had over 99% accuracy and fitted well to other subsamples of the data. However, the algorithm may not have as high of accuracy on other samples, particularly ones with different subjects.

Overall, it is interesting to see how monitors are affected by the quality of an exercise and are able to predict the error made which is an important indicator for health and fitness as it is not just the quantity of exercise that can be collected and analyzed but also the quality.
